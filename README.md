# ğŸ‰**Awesome-SLM**ğŸ‰

## ğŸŒ± How to Contribute
We are welcome contributions from researchers. For detailed guidelines on how to contribute, please see our [CONTRIBUTING.md](CONTRIBUTING.md) file.

## ğŸ“œ Contents
- [ğŸ‰**Awesome SLM**ğŸ‰](#Awesome-SLM)
  - [ğŸŒ± How to Contribute](#-how-to-contribute)
  - [ğŸ“œ Contents](#-contents)
  - [ğŸ‘‹ Introduction](#-introduction)
  - [ğŸ”¥ Base Model](#-base-model)
  - [ğŸ’ª Pretrain datasets](#-Pretrain-dataset)
  - [ğŸ’¡ SFT datasets](#-SFT-dataset)
  - [ğŸ”§ synthetic datasets](#-synthetic-dataset)
  - [ğŸ“¦ preference dataset](#-preference-dataset)
  - [ğŸŒˆ benchmark](#-benchmark)
 
## ğŸ‘‹ Introduction


## ğŸ”¥ Base Model
1. OPT-series [paper] [code] [model]
  - release time: 2022/06
  - organzation: meta
  - model size: 125M, 350M, 1.3B, 2.7B, 6.7B, 13B, 30B, 66B, 175B
  1. è®­ç»ƒæ•°æ®ï¼šè¯¥æ¨¡å‹ä½¿ç”¨äº†å¹¿æ³›çš„è®­ç»ƒæ•°æ®é›†ï¼ŒåŒ…æ‹¬RoBERTaä½¿ç”¨çš„æ•°æ®é›†ã€The Pileä»¥åŠPushShift.ioçš„Redditæ•°æ®ï¼Œæ•°æ®é‡ä¸º180B tokensã€‚å¯¹æ•°æ®è¿›è¡Œäº†å»é‡å¤„ç†ï¼Œè¿™äº›æ•°æ®ä¸»è¦æ˜¯è‹±è¯­æ–‡æœ¬ï¼Œä½¿ç”¨GPT-2çš„BPEåˆ†è¯å™¨ã€‚
  2. è®­ç»ƒç­–ç•¥ï¼šæ¨¡å‹è®­ç»ƒä½¿ç”¨äº†AdamWä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡é‡‡ç”¨çº¿æ€§è°ƒåº¦ï¼Œä»é›¶é€æ­¥å‡è‡³æœ€å¤§å€¼ï¼Œç„¶åéšç€è®­ç»ƒè¿›ç¨‹é€æ¸ä¸‹é™ã€‚æ­¤å¤–ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨äº†è¾ƒå¤§çš„batch size
  3. æ³¨æ„åŠ›æœºåˆ¶ï¼šä»…è§£ç å™¨çš„é¢„è®­ç»ƒtransformeræ¨¡å‹ï¼Œé‡‡ç”¨å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿ç”¨äº¤æ›¿dense and locally banded sparse attention
  4. æ¨¡å‹å±‚æ•°å’ŒBlockç±»å‹ï¼šæ¨¡å‹çš„æ¶æ„å’Œè¶…å‚æ•°ä¸»è¦éµå¾ªGPT-3çš„è®¾è®¡

![alt text](image.png)

2. Pythia [paper] [code] [model]
  - release time: 2023/06
  - organzation: meta
  - model size: 70M, 160M, 410M, 1.0B, 1.4B, 2.8B, 6.9B, 12B
  - æ¨¡å‹ç»“æ„ï¼š
  1. è®­ç»ƒæ•°æ®ï¼šè®­ç»ƒæ•°æ®ä½¿ç”¨çš„æ˜¯Pileæ•°æ®é›†ï¼Œæ•°æ®ä¸ºå…¨è‹±æ–‡ï¼Œç»å»é‡å¤„ç†åæ•°æ®é‡å¤§å°ä¸º 207B
  2. è®­ç»ƒç­–ç•¥ï¼šä½¿ç”¨GPT-NeoXåº“è¿›è¡Œè®­ç»ƒï¼Œé‡‡ç”¨Adamä¼˜åŒ–å™¨ï¼Œå¹¶åˆ©ç”¨é›¶å†—ä½™ä¼˜åŒ–ï¼ˆZeROï¼‰å’Œæ•°æ®å¹¶è¡Œã€å¼ é‡å¹¶è¡Œçš„æ–¹æ³•æ¥ä¼˜åŒ–æ€§èƒ½ã€‚
  3. æ³¨æ„åŠ›æœºåˆ¶ï¼šé‡‡ç”¨å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œdense attentionï¼Œä½¿ç”¨æ—‹è½¬åµŒå…¥ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨Flash AttentionæŠ€æœ¯æ¥æé«˜è®¾å¤‡ååé‡
  4. æ¨¡å‹å±‚æ•°å’ŒBlockç±»å‹ï¼šæ¨¡å‹çš„æ¶æ„å’Œè¶…å‚æ•°ä¸»è¦éµå¾ªGPT-3çš„è®¾è®¡

![alt text](image-1.png)

3. phi-1 [paper] [code] [model]
  - release time: 2023/06
  - organzation: mircosoft
  - model size: 1.42B
  - æ¨¡å‹ç»“æ„ï¼š
  1. è®­ç»ƒæ•°æ®ï¼š  æ•°æ®é›†å¤„ç†æ–¹é¢æ¯”è¾ƒæœ‰ç‰¹è‰²ï¼Œæå‡ºäº†æ•™ç§‘ä¹¦çº§æ•°æ®ï¼ŒåŒ…å«ä»The Stackå’ŒStackOverflowä¸­ç­›é€‰å‡ºçš„å­é›†ï¼ˆçº¦6B tokensï¼‰ã€ç”±GPT-3.5ç”Ÿæˆçš„Pythonæ•™ç§‘ä¹¦ï¼ˆå°‘äº1Bçš„tokensï¼‰ã€çº¦180Mçš„Pythonç»ƒä¹ å’Œè§£å†³æ–¹æ¡ˆçš„tokens
  2. è®­ç»ƒç­–ç•¥ï¼šphi-1-baseæ¨¡å‹åœ¨CodeTextbookæ•°æ®é›†ï¼ˆè¿‡æ»¤åçš„ä»£ç è¯­è¨€æ•°æ®é›†å’Œåˆæˆæ•™ç§‘ä¹¦æ•°æ®é›†ï¼‰ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œä½¿ç”¨AdamWä¼˜åŒ–å™¨ã€çº¿æ€§é¢„çƒ­çº¿æ€§è¡°å‡å­¦ä¹ ç‡è°ƒåº¦ã€attentionå’Œæ®‹å·®dropoutå‡ä¸º0.1ï¼Œbatch sizeä¸º1024
  3. æ³¨æ„åŠ›æœºåˆ¶ï¼šä½¿ç”¨äº†ä»…è§£ç å™¨transformerçš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶åœ¨é¢„è®­ç»ƒå’Œå¾®è°ƒè¿‡ç¨‹ä¸­ä½¿ç”¨äº†flashattentionæ¥æé«˜æ•ˆç‡
  4. æ¨¡å‹å±‚æ•°å’ŒBlockç±»å‹ï¼š
  æ¨¡å‹åŒ…å«24å±‚ï¼Œä½¿ç”¨å¹¶è¡Œé…ç½®çš„ MHA å’Œ MLP å±‚ï¼Œæ¯ä¸ªBlockåŒ…æ‹¬ä»¥ä¸‹éƒ¨åˆ†ï¼š
      éšè—å±‚å¤§å°ï¼š2048
      æ³¨æ„åŠ›å¤´æ•°ï¼š32
      æœ€å¤§ä½ç½®åµŒå…¥ï¼š2048
      ä½ç½®åµŒå…¥ç±»å‹ï¼šæ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆrotaryï¼‰
      æ®‹å·®è¿æ¥ï¼šgpt-j-residual

4. phi-1_5 [paper] [model]
  - release time: 2023/09
  - organzation: mircosoft
  - model size: 1.42B
  - æ¨¡å‹ç»“æ„ï¼š
  1. è®­ç»ƒæ•°æ®ï¼š  phi-1çš„è®­ç»ƒæ•°æ®ï¼ˆ7B tokensï¼‰+æ–°åˆ›å»ºçš„åˆæˆâ€œæ•™ç§‘ä¹¦â€æ•°æ®ï¼ˆçº¦20B tokensï¼‰ï¼Œç”¨äºæ•™æˆå¸¸è¯†æ¨ç†å’Œä¸–ç•Œé€šç”¨çŸ¥è¯†ï¼ˆç§‘å­¦ã€æ—¥å¸¸æ´»åŠ¨ã€å¿ƒæ™ºç†è®ºç­‰ï¼‰
  2. è®­ç»ƒç­–ç•¥ï¼šä»éšæœºåˆå§‹åŒ–å¼€å§‹è®­ç»ƒphi-1.5ï¼Œä½¿ç”¨å¸¸æ•°å­¦ä¹ ç‡2e-4ï¼ˆæ— é¢„çƒ­ï¼‰ï¼Œæƒé‡è¡°å‡ï¼š0.1ã€‚Adamä¼˜åŒ–å™¨ï¼ŒåŠ¨é‡å‚æ•°ä¸º0.9å’Œ0.98ã€‚æ··åˆç²¾åº¦è®­ç»ƒä½¿ç”¨fp16å’ŒDeepSpeed ZeRO Stage 2ï¼Œæ‰¹é‡å¤§å°2048ã€‚
  3. æ³¨æ„åŠ›æœºåˆ¶ï¼šä½¿ç”¨äº†ä»…è§£ç å™¨transformerçš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¹¶åœ¨é¢„è®­ç»ƒå’Œå¾®è°ƒè¿‡ç¨‹ä¸­ä½¿ç”¨äº†flashattentionæ¥æé«˜æ•ˆç‡
  4. æ¨¡å‹å±‚æ•°å’ŒBlockç±»å‹ï¼šï¼ˆå’Œphi-1ç›¸åŒï¼‰
  æ¨¡å‹åŒ…å«24å±‚ï¼Œä½¿ç”¨å¹¶è¡Œé…ç½®çš„ MHA å’Œ MLP å±‚ï¼Œæ¯ä¸ªBlockåŒ…æ‹¬ä»¥ä¸‹éƒ¨åˆ†ï¼š
      éšè—å±‚å¤§å°ï¼š2048
      æ³¨æ„åŠ›å¤´æ•°ï¼š32
      æ³¨æ„åŠ›å¤´ç»´åº¦ï¼š64
      æœ€å¤§ä½ç½®åµŒå…¥ï¼š2048
      ä½ç½®åµŒå…¥ç±»å‹ï¼šæ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆrotaryï¼‰
      æ®‹å·®è¿æ¥ï¼šgpt-j-residual

5. phi-2 [paper][model]
  - release time: 2023/12
  - organzation: mircosoft
  - model size: 2.78B
  - æ¨¡å‹ç»“æ„ï¼š
  1. è®­ç»ƒæ•°æ®ï¼š ä¸phi-1.5ç›¸åŒï¼Œæ•°æ®æºåŸºäºphi-1.5ï¼Œå¹¶å¢åŠ äº†ç”±å„ç§NLPåˆæˆæ–‡æœ¬å’Œè¿‡æ»¤ç½‘ç«™ï¼ˆå‡ºäºå®‰å…¨å’Œæ•™è‚²ä»·å€¼ï¼‰ç»„æˆçš„æ–°æ•°æ®æº250B tokensï¼Œè®­ç»ƒtokensä¸º1.4T tokens
  2. è®­ç»ƒç­–ç•¥ï¼šæ²¡æœ‰è¯¦ç»†æ˜ç¡®
  3. æ³¨æ„åŠ›æœºåˆ¶ï¼šæ²¡æœ‰è¯¦ç»†æ˜ç¡®
  4. æ¨¡å‹å±‚æ•°å’ŒBlockç±»å‹ï¼šæ²¡æœ‰è¯¦ç»†æ˜ç¡®

6. phi-3-series [paper] [model]
  - release time: 2024/04
  - organzation: mircosoft
  - model series: Phi-3-mini-4k-instruct, Phi-3-mini-128k-instruct
  - model size: 3.82B
  - æ¨¡å‹ç»“æ„ï¼š
  1. è®­ç»ƒæ•°æ®ï¼š è®­ç»ƒæ•°æ®é›†æ˜¯phi-2æ•°æ®é›†çš„å‡çº§ç‰ˆæœ¬ï¼ŒåŒ…æ‹¬ç»è¿‡ä¸¥æ ¼è¿‡æ»¤çš„å…¬å¼€å¯ç”¨çš„ç½‘é¡µæ•°æ®å’Œåˆæˆæ•°æ®ï¼Œè®­ç»ƒtokensä¸º3.3T tokens
  2. è®­ç»ƒç­–ç•¥ï¼š
  é¢„è®­ç»ƒåˆ†ä¸ºä¸¤ä¸ªä¸è¿ç»­ä¸”é¡ºåºè¿›è¡Œçš„é˜¶æ®µï¼š1.ä¸»è¦ä½¿ç”¨ç½‘ç»œæ¥æºçš„æ•°æ®ï¼Œæ—¨åœ¨æ•™æˆæ¨¡å‹é€šç”¨çŸ¥è¯†å’Œè¯­è¨€ç†è§£ã€‚2.åˆå¹¶äº†æ›´å¤šç»è¿‡ä¸¥æ ¼è¿‡æ»¤çš„ç½‘ç»œæ•°æ®å’Œä¸€äº›åˆæˆæ•°æ®ï¼Œæ—¨åœ¨æ•™æˆæ¨¡å‹é€»è¾‘æ¨ç†å’Œå„ç§ç‰¹æ®ŠæŠ€èƒ½
  åè®­ç»ƒï¼šåŒ…æ‹¬ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å’Œç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDPOï¼‰ä¸¤ä¸ªé˜¶æ®µã€‚SFTæ•°æ®é›†è¦†ç›–å¤šç§é¢†åŸŸçš„é«˜è´¨é‡æ•°æ®ï¼ŒDPOæ•°æ®é›†åˆ™ç”¨äºè°ƒæ•´æ¨¡å‹è¡Œä¸º
  3. æ³¨æ„åŠ›æœºåˆ¶ï¼šä½¿ç”¨äº†ä»…è§£ç å™¨transformerçš„åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›æœºåˆ¶ï¼Œé»˜è®¤ä¸Šä¸‹æ–‡é•¿åº¦ä¸º4Kï¼Œä½¿ç”¨LongRopeæŠ€æœ¯æ‰©å±•åˆ°128Kï¼Œä½¿ç”¨Flash AttentionåŠ é€Ÿè®­ç»ƒ
  4. æ¨¡å‹å±‚æ•°å’ŒBlockç±»å‹ï¼šï¼ˆ Llama-2 ç±»ä¼¼çš„å—ç»“æ„ï¼‰
  æ¨¡å‹åŒ…å«32å±‚ï¼Œæ¯ä¸ªBlockåŒ…æ‹¬ä»¥ä¸‹éƒ¨åˆ†ï¼š
      éšè—å±‚å¤§å°ï¼š3072
      æ³¨æ„åŠ›å¤´æ•°ï¼š32
      æ³¨æ„åŠ›å¤´ç»´åº¦ï¼š64
      æœ€å¤§ä½ç½®åµŒå…¥ï¼š2048
      ä½ç½®åµŒå…¥ç±»å‹ï¼šæ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆrotaryï¼‰
      æ®‹å·®è¿æ¥ï¼šgpt-j-residual

7. Tinyllama [paper] [model]
  - release time: 2024/01
  - organzation: æ–°åŠ å¡ç§‘æŠ€ä¸è®¾è®¡å¤§å­¦
  - model size: 1.1B
  - æ¨¡å‹ç»“æ„ï¼š
  1. è®­ç»ƒæ•°æ®ï¼š è®­ç»ƒæ•°æ®ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼š SlimPajamaï¼šè¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„è¯­æ–™åº“ï¼Œä¸“é—¨ç”¨äºè®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ã€‚å®ƒç”±RedPajamaè¡ç”Ÿè€Œæ¥ï¼Œå¹¶ç»è¿‡é¢å¤–çš„æ¸…æ´—å’Œå»é‡è¿‡ç¨‹ã€‚åŸå§‹çš„RedPajamaè¯­æ–™åº“åŒ…å«è¶…è¿‡1.2ä¸‡äº¿tokensã€‚ç»è¿‡è¿‡æ»¤åï¼ŒSlimPajamaä¿ç•™äº†åŸå§‹tokensçš„50%ï¼›StarCoderè®­ç»ƒæ•°æ®é›†ï¼šè¿™ä¸ªæ•°æ®é›†ç”¨äºè®­ç»ƒStarCoderï¼ŒåŒ…å«86ç§ç¼–ç¨‹è¯­è¨€çš„æ•°æ®ï¼Œé™¤äº†ä»£ç æ•°æ®å¤–ï¼Œè¿˜åŒ…æ‹¬GitHubé—®é¢˜å’Œæ¶‰åŠè‡ªç„¶è¯­è¨€çš„æ–‡æœ¬-ä»£ç å¯¹ã€‚ä¸ºé¿å…é‡å¤ï¼ŒSlimPajamaä¸­ç§»é™¤äº†GitHubå­é›†ï¼Œåªä»StarCoderè®­ç»ƒæ•°æ®é›†ä¸­é‡‡æ ·ä»£ç ç›¸å…³æ•°æ®.åˆå¹¶è¿™ä¸¤ä¸ªæ•°æ®é›†åï¼Œå¾—åˆ°å¤§çº¦9500äº¿tokensè¿›è¡Œé¢„è®­ç»ƒï¼Œæ€»å…±å¤„ç†äº†3ä¸‡äº¿tokens
  2. è®­ç»ƒç­–ç•¥ï¼šAdamwä¼˜åŒ–å™¨ï¼ŒåŸºäºlit-gptæ„å»ºæ¡†æ¶ã€‚TinyLlamaçš„é¢„è®­ç»ƒåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼š 
  åŸºç¡€é¢„è®­ç»ƒï¼šä½¿ç”¨SlimPajamaæ•°æ®è®­ç»ƒ1.5ä¸‡äº¿tokensï¼Œä¸»è¦å‘å±•æ¨¡å‹çš„å¸¸è¯†æ¨ç†èƒ½åŠ›ã€‚
  æŒç»­é¢„è®­ç»ƒï¼šç»“åˆSlimPajamaæ•°æ®å’ŒStarCoderã€Proof Pileç­‰ä»£ç å’Œæ•°å­¦å†…å®¹ï¼Œä»¥åŠSkypileçš„ä¸­æ–‡æ•°æ®ï¼Œåˆ†åˆ«é’ˆå¯¹ä¸€èˆ¬åº”ç”¨ã€æ•°å­¦å’Œç¼–ç ä»»åŠ¡ä»¥åŠä¸­æ–‡å¤„ç†è¿›è¡ŒæŒç»­é¢„è®­ç»ƒ
  3. æ³¨æ„åŠ›æœºåˆ¶ï¼šä½¿ç”¨åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿ç”¨æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰,ä½¿ç”¨Flash Attention2åŠ é€Ÿè®­ç»ƒ
  4. æ¨¡å‹å±‚æ•°å’ŒBlockç±»å‹ï¼šæ¨¡å‹å…±22å±‚
  éšè—å±‚å¤§å°ï¼š2048
  ä¸­é—´éšè—å±‚å¤§å°ï¼š5632
  ä¸Šä¸‹æ–‡é•¿åº¦ï¼š2048
  æ³¨æ„åŠ›å¤´æ•°ï¼š32
  è¯æ±‡è¡¨å¤§å°ï¼š32000
  æ¿€æ´»å‡½æ•°ï¼šä½¿ç”¨SwiGLUï¼Œå³Swishæ¿€æ´»å‡½æ•°å’Œé—¨æ§çº¿æ€§å•å…ƒï¼ˆGLUï¼‰çš„ç»“åˆ
  é¢„å½’ä¸€åŒ–å’ŒRMSå½’ä¸€åŒ–ï¼šåœ¨æ¯ä¸ªTransformerå­å±‚çš„è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–

8. MiniCPM-series [paper][code] [model]
  - release time: 2024/02
  - organzation: openbmb
  - model series: MiniCPM-1B-sft-bf16, MiniCPM-2B-sft-bf16, MiniCPM-2B-sft-fp32, MiniCPM-2B-128k, MiniCPM-MoE-8x2B
  - model size: 1.2B, 2.4B, 8X2.4B (excluding embeddings)

9. H2O-Danube-1.8B [paper] [code] [model]
  - release time: 2024/04
  - organzation: h2oai
  - model series: h2o-danube2-1.8b-base, h2o-danube2-1.8b-sft, h2o-danube2-1.8b-chat
  - model size: 1.8B

10. csg-wukong-series[model]
  - release time: 2024/04
  - organzation: opencsg
  - model series: csg-wukong-1B, csg-wukong-1B-VL, csg-wukong-1B-chat
  - model size: 1B

11. CT-LLM-Base[paper] [code] [model]
  - release time: 2024/04
  - organzation: Peking University
  - model series: CT-LLM-Base
  - model size: 2B

12. Qwen-series[paper] [code] [model]
  - release time: 2023/08
  - organzation: Alibaba Cloud
  - model series: Qwen-1.8B, Qwen-7B, Qwen-14B, and Qwen-72B, Qwen-1.8B-Chat, Qwen-7B-Chat, Qwen-14B-Chat, Qwen-72B-Chat
  - model size: 1.8B,7B,14B,72B

13. Qwen2-series[paper] [code] [model]
  - release time: 2024/06
  - organzation: Alibaba Cloud
  - model series: Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, Qwen2-57B-A14B, Qwen2-72B
  - model size: 0.5B,7B,A14B,72B

14. Gemma-series[paper] [code] [model]
  - release time: 2024/02
  - organzation: Google
  - model series: gemma-2b, gemma-2b-it, gemma-7b, gemma-7b-it, gemma-2-9b,gemma-2-9b-it,gemma-2-27b,gemma-2-27b-it
  - model size: 2B,7B,27B

15. OpenELM-series[paper] [code] [model]
  - release time: 2024/04
  - organzation: apple
  - model series: OpenELM-270M, OpenELM-450M, OpenELM-1.1B, OpenELM-3B,OpenELM-270M-Instruct,OpenELM-450M-Instruct,OpenELM-1.1B-Instruct,OpenELM-3B-Instruct
  - model size: 0.27B,0.45B,1.1B,3B

16. Sheared-LLaMA-series[paper] [code] [model]
  - release time: 2023/10
  - organzation: Princeton NLP group
  - model series:  Sheared-LLaMA-1.3B, Sheared-LLaMA-2.7B,Sheared-LLaMA-1.3B-Pruned, Sheared-LLaMA-2.7B-Pruned,Sheared-LLaMA-1.3B-ShareGPT, Sheared-LLaMA-2.7B-ShareGPT
  - model size: 1.3B,2.7B

17. SlimPajama-DC[paper] [code] [model]
  - release time: 2023/09
  - organzation: cerebras
  - model series:  SlimPajama-DC-1.3B
  - model size: 1.3B

18. RedPajama [code] [model]
  - release time: 2023/05
  - organzation: Together Computer.
  - model series: RedPajama-INCITE-Base-3B-v1, RedPajama-INCITE-Instruct-3B-v1,RedPajama-INCITE-Chat-3B-v1
  - model size: 1.3B

19. OLMo[paper] [code] [model]
  - release time: 2024/02
  - organzation: allenai
  - model series:  OLMo-1B,OLMo-7B,OLMo-7B-Twin-2T
  - model size: 1B,7B

20. Cerebras-GPT-series[paper] [model]
  - release time: 2023/04
  - organzation: cerebras
  - model series:  Cerebras-GPT-111M,Cerebras-GPT-256M,Cerebras-GPT-590M,Cerebras-GPT-11.3B,Cerebras-GPT-2.7B,Cerebras-GPT-6.7B,Cerebras-GPT-111M,Cerebras-GPT-13B
  - model size: 111M, 256M, 590M, 1.3B, 2.7B, 6.7B, 13B


- Qwen-series[[paper](https://arxiv.org/abs/2309.16609)] [[code](https://github.com/QwenLM/Qwen)] 
[[model](https://huggingface.co/Qwen)]
    - release time: 2023/08
    - organzation: Alibaba Cloud
    - model series: Qwen-1.8B, Qwen-7B, Qwen-14B, and Qwen-72B, Qwen-1.8B-Chat, Qwen-7B-Chat, Qwen-14B-Chat, Qwen-72B-Chat
    - model size: 1.8B,7B,14B,72B

- Qwen2-series[[paper](https://qwenlm.github.io/zh/blog/qwen2/)] [[code](https://github.com/QwenLM/Qwen2)] [[model](https://huggingface.co/Qwen)]
    - release time: 2024/06
    - organzation: Alibaba Cloud
    - model series: Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, Qwen2-57B-A14B, Qwen2-72B
    - model size: 0.5B,7B,A14B,72B

- Gemma-series[[paper](https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf)] [[code](https://github.com/google-deepmind/gemma)] [[model](https://huggingface.co/google/gemma-2b)]
    - release time: 2024/02
    - organzation: Google
    - model series: gemma-2b, gemma-2b-it, gemma-7b, gemma-7b-it, gemma-2-9b,gemma-2-9b-it,gemma-2-27b,gemma-2-27b-it
    - model size: 2B,7B,27B

- OpenELM-series[[paper](https://arxiv.org/abs/2404.14619)] [[code](https://github.com/apple/corenet)] [[model](https://huggingface.co/collections/apple/openelm-instruct-models-6619ad295d7ae9f868b759c)]
    - release time: 2024/04
    - organzation: apple
    - model series: OpenELM-270M, OpenELM-450M, OpenELM-1.1B, OpenELM-3B,OpenELM-270M-Instruct,OpenELM-450M-Instruct,OpenELM-1.1B-Instruct,OpenELM-3B-Instruct
    - model size: 0.27B,0.45B,1.1B,3B

 - Sheared-LLaMA-series[[paper](https://arxiv.org/abs/2310.06694)] [[code](https://github.com/princeton-nlp/LLM-Shearing)] [[model](https://huggingface.co/princeton-nlp/Sheared-LLaMA-1.3B)]
    - release time: 2023/10
    - organzation: Princeton NLP group
    - model series:  Sheared-LLaMA-1.3B, Sheared-LLaMA-2.7B,Sheared-LLaMA-1.3B-Pruned, Sheared-LLaMA-2.7B-Pruned,Sheared-LLaMA-1.3B-ShareGPT, Sheared-LLaMA-2.7B-ShareGPT
    - model size: 1.3B,2.7B


## ğŸ’ª Pretrain Datasets
- SlimPajama-627B [[paper](https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama)] [[code](https://github.com/Cerebras/modelzoo/tree/main/src/cerebras/modelzoo/data_preparation/nlp/slimpajama)] [[dataset](https://huggingface.co/datasets/cerebras/SlimPajama-627B)]
  - release time: 2023/06
  - dataset size: 895 GB
  - token size: 627B
  - language: Primarily English, with some non-English files in Wikipedia
  

- dolma [[paper](https://arxiv.org/abs/2402.00159)] [[code](https://github.com/allenai/dolma)] [[dataset](https://huggingface.co/datasets/allenai/dolma)]
  - release time: 2024/04
  - dataset size: 4.5TB
  - token size: 1.7T
  - language: Primarily English, with some non-English files in Wikipedia

- RedPajama-Data-1T [[paper](https://arxiv.org/pdf/1906.02285.pdf)] [[code](https://github.com/togethercomputer/RedPajama-Data)] [[dataset](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T)]
  - release time: 2023/04
  - token size: 627B

- C4 [[paper](https://www.tensorflow.org/datasets/catalog/c4)] [[code](https://github.com/allenai/c4-documentation)] [[dataset](https://huggingface.co/datasets/c4)]
  - release time: 2022/01
  - dataset size: en: 305GB, en.noclean: 2.3TB, en.noblocklist: 380GB, realnewslike: 15GB, multilingual (mC4): 9.7TB (108 subsets, one per language)


## ğŸ’¡ SFT Datasets
- ultrachat [[code](https://github.com/thunlp/UltraChat)] [[dataset](https://huggingface.co/datasets/stingning/ultrachat)]
  - release time: 2023/04
  - dataset size: 2.5GB
  - language: en

- ultrachat_200k [[code](https://github.com/thunlp/UltraChat)] [[dataset](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k)]
  - release time: 2023/10
  - dataset size: 1.6GB
  - language: en


## ğŸ”§ synthetic datasets
- cosmopedia [[code](https://github.com/thunlp/UltraChat)] [[dataset](https://huggingface.co/datasets/HuggingFaceTB/cosmopedia)]
  - release time: 2024/02
  - dataset size: 92.2GB
  - language: en


## ğŸ“¦ preference dataset
- UltraFeedback [[code](https://github.com/thunlp/UltraChat)] [[dataset](https://huggingface.co/datasets/openbmb/UltraFeedback)]
  - release time: 2023/09
  - dataset size: 0.94GB
  - language: en


## ğŸŒˆ benchmark
